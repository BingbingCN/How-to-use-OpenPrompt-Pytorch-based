{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Dataset..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (C:/Users/cq906/.cache/huggingface/datasets/super_glue/cb/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7ba36fbac83494983fc19426104fcbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'It was a complex language. Not written down but handed down. One might say it was peeled down.', 'hypothesis': 'the language was peeled down', 'idx': 0, 'label': 0}\n",
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"It was a complex language. Not written down but handed down. One might say it was peeled down.\",\n",
      "  \"text_b\": \"the language was peeled down\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate,ManualVerbalizer\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt import PromptForClassification\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "print('Load Dataset..')\n",
    "raw_dataset = load_dataset('super_glue', 'cb')\n",
    "print(raw_dataset['train'][0])\n",
    "\n",
    "dataset={}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model..\n",
      "Build Template..\n",
      "[[{'loss_ids': 0,\n",
      "   'shortenable_ids': 1,\n",
      "   'text': 'It was a complex language. Not written down but handed down. One '\n",
      "           'might say it was peeled down.'},\n",
      "  {'loss_ids': 0, 'shortenable_ids': 0, 'text': ' Question:'},\n",
      "  {'loss_ids': 0,\n",
      "   'shortenable_ids': 1,\n",
      "   'text': ' the language was peeled down'},\n",
      "  {'loss_ids': 0, 'shortenable_ids': 0, 'text': '? Is it correct?'},\n",
      "  {'loss_ids': 1, 'shortenable_ids': 0, 'text': '<mask>'},\n",
      "  {'loss_ids': 0, 'shortenable_ids': 0, 'text': '.'}],\n",
      " {'guid': 0, 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cq906\\File\\Anaconda\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Load Model..')\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")\n",
    "print('Build Template..')\n",
    "template_text = '{\"placeholder\":\"text_a\"} Question: {\"placeholder\":\"text_b\"}? Is it correct? {\"mask\"}.'\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "'''\n",
    "Return two list 1-template-text,2-label and guid\n",
    "'''\n",
    "pprint(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [94, 47, 3, 9, 1561, 1612, 5, 933, 1545, 323, 68, 14014, 323, 5, 555, 429, 497, 34, 47, 158, 400, 26, 323, 5, 11860, 10, 8, 1612, 47, 158, 400, 26, 323, 3, 58, 27, 7, 34, 2024, 58, 32099, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [0, 32099, 0], 'loss_ids': [0, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Note that when t5 is used for classification, we only need to pass <pad> <extra_id_0> <eos> to decoder.\n",
    "# The loss is calcaluted at <extra_id_0>. Thus passing decoder_max_length=3 saves the space\n",
    "wrapped_t5tokenizer=WrapperClass(max_seq_length=128,\n",
    "                                 decoder_max_length=3,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 truncate_method='head')\n",
    "tokenized_example=wrapped_t5tokenizer.tokenize_one_example(wrapped_example,teacher_forcing=False)\n",
    "print(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 1069.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:00<00:00, 1515.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 250/250 [00:00<00:00, 1062.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model_inputs = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print('Processing: ',split.title())\n",
    "    model_inputs[split] = []\n",
    "    for sample in tqdm(dataset[split]):\n",
    "        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
    "        model_inputs[split].append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Dataloader..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 250it [00:00, 1158.91it/s]\n",
      "tokenizing: 56it [00:00, 1322.92it/s]\n",
      "tokenizing: 250it [00:00, 1084.20it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Build Dataloader..')\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=16,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "valid_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=16,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[4273]],\n",
      "\n",
      "        [[ 150]],\n",
      "\n",
      "        [[2087]]])\n",
      "tensor([[-1.1512, -1.8715, -0.6351],\n",
      "        [-2.9707, -0.3543, -1.3981]])\n"
     ]
    }
   ],
   "source": [
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=3,\n",
    "                        label_words=[[\"yes\"], [\"no\"], [\"maybe\"]])\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
    "print(myverbalizer.process_logits(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptModel=PromptForClassification(plm=plm,template=mytemplate,verbalizer=myverbalizer,freeze_plm=False)\n",
    "PromptModel=PromptModel.cuda()\n",
    "loss_func=torch.nn.CrossEntropyLoss()\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in PromptModel.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in PromptModel.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer=torch.optim.AdamW(optimizer_grouped_parameters,lr=1.5e-4)\n",
    "writer=SummaryWriter(log_dir='log_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 16/16 [00:10<00:00,  1.51it/s, loss=0.197]\n",
      "epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.76it/s, loss=0.824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s, loss=0.286] \n",
      "epoch 2: 100%|██████████| 4/4 [00:00<00:00,  6.82it/s, loss=0.0678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s, loss=0.609]  \n",
      "epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.77it/s, loss=1.2]   \n",
      "epoch 4: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s, loss=0.00548]\n",
      "epoch 4: 100%|██████████| 4/4 [00:00<00:00,  6.81it/s, loss=0.000546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5: 100%|██████████| 16/16 [00:09<00:00,  1.71it/s, loss=0.00705] \n",
      "epoch 5: 100%|██████████| 4/4 [00:00<00:00,  6.36it/s, loss=0.325] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6: 100%|██████████| 16/16 [00:09<00:00,  1.67it/s, loss=0.00115]\n",
      "epoch 6: 100%|██████████| 4/4 [00:00<00:00,  6.47it/s, loss=0.0162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7: 100%|██████████| 16/16 [00:09<00:00,  1.75it/s, loss=0.000661]\n",
      "epoch 7: 100%|██████████| 4/4 [00:00<00:00,  6.54it/s, loss=0.00572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8: 100%|██████████| 16/16 [00:09<00:00,  1.75it/s, loss=0.000236]\n",
      "epoch 8: 100%|██████████| 4/4 [00:00<00:00,  6.52it/s, loss=1.02]  \n",
      "epoch 9: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s, loss=9.56e-5] \n",
      "epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.68it/s, loss=0.104] \n",
      "epoch 10: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=0.00345] \n",
      "epoch 10: 100%|██████████| 4/4 [00:00<00:00,  6.64it/s, loss=1.02]  \n",
      "epoch 11: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=4.47e-5] \n",
      "epoch 11: 100%|██████████| 4/4 [00:00<00:00,  6.55it/s, loss=0.151] \n",
      "epoch 12: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=0.000157]\n",
      "epoch 12: 100%|██████████| 4/4 [00:00<00:00,  6.39it/s, loss=1.45]  \n",
      "epoch 13: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=5.15e-5] \n",
      "epoch 13: 100%|██████████| 4/4 [00:00<00:00,  6.62it/s, loss=2.74e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=0.000708]\n",
      "epoch 14: 100%|██████████| 4/4 [00:00<00:00,  6.60it/s, loss=1.24]   \n",
      "epoch 15: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=0.000138]\n",
      "epoch 15: 100%|██████████| 4/4 [00:00<00:00,  6.56it/s, loss=0.184]\n",
      "epoch 16: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=0.000298]\n",
      "epoch 16: 100%|██████████| 4/4 [00:00<00:00,  6.48it/s, loss=6.03e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17: 100%|██████████| 16/16 [00:09<00:00,  1.74it/s, loss=0.000795]\n",
      "epoch 17: 100%|██████████| 4/4 [00:00<00:00,  6.55it/s, loss=0.000708]\n",
      "epoch 18: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s, loss=0.000168]\n",
      "epoch 18: 100%|██████████| 4/4 [00:00<00:00,  6.58it/s, loss=0.0446] \n",
      "epoch 19: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s, loss=0.000298]\n",
      "epoch 19: 100%|██████████| 4/4 [00:00<00:00,  6.74it/s, loss=0.0107]\n",
      "epoch 20: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s, loss=0.002]   \n",
      "epoch 20: 100%|██████████| 4/4 [00:00<00:00,  6.69it/s, loss=0.00581]\n"
     ]
    }
   ],
   "source": [
    "best_loss=9999999\n",
    "for epoch in range(20):\n",
    "    train_loss = 0\n",
    "    PromptModel.train()\n",
    "    par=tqdm(train_dataloader)\n",
    "    for step, inputs in enumerate(par):\n",
    "        inputs.cuda()\n",
    "        logits = PromptModel(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        par.set_description('epoch {}'.format(epoch+1))\n",
    "        par.set_postfix(loss=loss.item())\n",
    "    writer.add_scalars('Loss',{'train':train_loss/len(train_dataloader)},epoch+1)\n",
    "    PromptModel.eval()\n",
    "    val_loss=0\n",
    "    par=tqdm(valid_dataloader)\n",
    "    for step, inputs in enumerate(par):\n",
    "        inputs.cuda()\n",
    "        logits = PromptModel(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        val_loss += loss.item()\n",
    "        par.set_description('epoch {}'.format(epoch+1))\n",
    "        par.set_postfix(loss=loss.item())\n",
    "    val_loss=val_loss/len(valid_dataloader)\n",
    "    writer.add_scalars('Loss',{'valid':val_loss},epoch+1)\n",
    "\n",
    "    if val_loss<best_loss:\n",
    "        best_loss=val_loss\n",
    "        print('Save Model..')\n",
    "        torch.save(PromptModel.state_dict(),'t5-base-prompt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Best Model..\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Load Best Model..')\n",
    "PromptModel.cpu()\n",
    "PromptModel.load_state_dict(torch.load('t5-base-prompt.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy:94.64%\n"
     ]
    }
   ],
   "source": [
    "allpreds = []\n",
    "alllabels = []\n",
    "PromptModel.eval()\n",
    "for step, inputs in enumerate(valid_dataloader):\n",
    "\n",
    "    inputs = inputs.to('cpu')\n",
    "    logits = PromptModel(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print('Best Accuracy:{:.2f}%'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
